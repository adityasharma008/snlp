{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKmsqWZxhtE7",
        "outputId": "4adf718e-cb2b-4403-de1f-8525799ff7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKGeO-CLFzFV"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rppmdlmwhuiB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import operator\n",
        "from tensorflow import keras\n",
        "# from keras.utils import np_utils # This is no longer needed\n",
        "from tensorflow.keras.utils import to_categorical # Use this for one-hot encoding\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
        "# from tensorflow.keras.utils import to_categorical # Already imported above\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors as nn\n",
        "from matplotlib import pylab\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkPNrdTMF7_j"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCue2RJQ3ozs"
      },
      "source": [
        "Data set can be download from [here](https://drive.google.com/file/d/1tFhlcibsLZKbsxze_pXHCYeFnkQdBXhW/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uqL7A_4hyMa",
        "outputId": "7978210e-3717-417a-92cf-006fabf3d43a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMIKybcCKLqS",
        "outputId": "6a193391-c225-4bbf-d86b-4cb20aa21ae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Dataset\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/My Drive/Dataset/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suqjHjIqK7wz",
        "outputId": "40b87366-a7d3-474f-e7f3-901af90b1f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alice.txt                           vectors_cbow_150.txt\n",
            "GoogleNews-vectors-negative300.bin  vectors_cbow_32.txt\n",
            "Sentiment.csv                       vectors_skipgram_150.txt\n",
            "spam.csv                            vectors_skipgram_32.txt\n",
            "spam.xlsx\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qiSrkA2WKogQ"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/alice.txt'\n",
        "corpus = open(file_name).readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rTdaonJGB0y"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufWSkJFTKrR5",
        "outputId": "8c135237-f081-4e44-e030-196dbbf26ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27165, 2557)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove sentences with fewer than 3 words\n",
        "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
        "\n",
        "# Remove punctuation in text and fit tokenizer on entire corpus\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Convert text to sequence of integer values\n",
        "corpus = tokenizer.texts_to_sequences(corpus)\n",
        "n_samples = sum(len(s) for s in corpus) # Total number of words in the corpus\n",
        "V = len(tokenizer.word_index) + 1 # Total number of unique words in the corpus\n",
        "\n",
        "n_samples, V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2epXJEoXLTYR",
        "outputId": "d89949f4-b9f7-4235-fe3f-24ef89e7ff4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the', 1), ('and', 2), ('to', 3), ('a', 4), ('it', 5)]\n"
          ]
        }
      ],
      "source": [
        "#Integer mapping\n",
        "print(list((tokenizer.word_index.items()))[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3TSdpm0xLgOh"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "window_size = 2\n",
        "#window_size_corpus = 4\n",
        "\n",
        "# Set numpy seed for reproducible results\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyF9fmIoMFTM"
      },
      "source": [
        "**Skip gram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wnzhf08tLkDB"
      },
      "outputs": [],
      "source": [
        "# Prepare data for the skipgram model\n",
        "def generate_data_skipgram(corpus, window_size, V):\n",
        "    maxlen = window_size * 2\n",
        "    all_in = []\n",
        "    all_out = []\n",
        "    for words in corpus:\n",
        "        L = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            p = index - window_size\n",
        "            n = index + window_size + 1\n",
        "\n",
        "            in_words = []\n",
        "            labels = []\n",
        "            for i in range(p, n):\n",
        "                if i != index and 0 <= i < L:\n",
        "                    # Add the input word\n",
        "                    all_in.append(word)\n",
        "                    # Add one-hot of the context words\n",
        "                    all_out.append(to_categorical(words[i], V))\n",
        "\n",
        "    return (np.array(all_in), np.array(all_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A2TFfdvMx2p",
        "outputId": "1fb300e7-b351-4ee8-d7b2-eae265babbb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((94556,), (94556, 2557))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_skip, y_skip = generate_data_skipgram(corpus, window_size, V)\n",
        "X_skip.shape, y_skip.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXuGrjp5M0Uo",
        "outputId": "35c5453c-8ff9-46e3-a33e-a9b4d5e5da23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.int64(305), array([0., 0., 0., ..., 0., 0., 0.]))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_skip[1],y_skip[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "IHU2mLRqM7wQ",
        "outputId": "fd833c96-ecbd-4e6d-8db5-35ff2af95e68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">81,824</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2557</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">84,381</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │        \u001b[38;5;34m81,824\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2557\u001b[0m)           │        \u001b[38;5;34m84,381\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,205</span> (649.24 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m166,205\u001b[0m (649.24 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">166,205</span> (649.24 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m166,205\u001b[0m (649.24 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "skipgram = Sequential()\n",
        "dim =32\n",
        "# Add an Embedding layer\n",
        "skipgram.add(Embedding(input_dim=V,\n",
        "                       output_dim=dim,\n",
        "                       input_length=1,\n",
        "                       embeddings_initializer='glorot_uniform'))\n",
        "\n",
        "# Add a Reshape layer, which reshapes the output of the embedding layer (1,dim) to (dim,)\n",
        "skipgram.add(Reshape((dim, )))\n",
        "\n",
        "# Add a final Dense layer with the same size as in [1]\n",
        "skipgram.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "# Compile the model with a suitable loss function and select an optimizer.\n",
        "# Optimizer Adagrad was used in paper\n",
        "skipgram.compile(optimizer=keras.optimizers.Adam(),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "skipgram.build(input_shape=(None, 1))\n",
        "skipgram.summary()\n",
        "print(\"\")\n",
        "#skipgram_models.append(skipgram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLuMMZyVOEzp",
        "outputId": "7eef95a9-f44a-458a-f443-45cbb54ca8c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.0540 - loss: 7.0012\n",
            "Epoch 2/3\n",
            "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0629 - loss: 5.9558\n",
            "Epoch 3/3\n",
            "\u001b[1m1478/1478\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.0677 - loss: 5.8394\n",
            "\n"
          ]
        }
      ],
      "source": [
        "skipgram.fit(X_skip, y_skip, batch_size=64, epochs=3, verbose=1)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKmbL9swOQhp",
        "outputId": "44c5d31f-efc5-4c44-af7d-d9ec869733d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights = skipgram.get_weights()\n",
        "len(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ARw539jvPb-i"
      },
      "outputs": [],
      "source": [
        "embedding = weights[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiE463JEQFdB",
        "outputId": "e8190da5-b54e-4cef-82b4-2ece294ac040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2557, 32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C5Eu9KJQJ3Q",
        "outputId": "1ec3eb82-01a8-4d04-af1a-9e79886349a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.9326653 ,  0.9412425 ,  1.0245277 , ..., -0.41742665,\n",
              "       -0.47592288, -0.211484  ], dtype=float32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mjSkuRy5QQnx"
      },
      "outputs": [],
      "source": [
        "f = open(f\"vectors_skipgram_{len(embedding[0])}.txt\", \"w\")\n",
        "columns = [\"word\"] + [f\"value_{i+1}\" for i in range(embedding.shape[1])]\n",
        "f.write(\" \".join(columns))\n",
        "f.write(\"\\n\")\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  f.write(word)\n",
        "  f.write(\" \")\n",
        "  f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
        "  f.write(\"\\n\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn2ZM_dhalSn"
      },
      "source": [
        "**CBOW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "U8D5wLVTapNX"
      },
      "outputs": [],
      "source": [
        "def generate_data_cbow(corpus, window_size, V):\n",
        "  maxlen = window_size * 2\n",
        "  all_in = []\n",
        "  all_out = []\n",
        "  for words in corpus:\n",
        "      L = len(words)\n",
        "      for index, word in enumerate(words):\n",
        "          p = index - window_size\n",
        "          n = index + window_size + 1\n",
        "\n",
        "          context_words = []\n",
        "          for i in range(p, n):\n",
        "              if i != index and 0 <= i < L:\n",
        "                  context_words.append(words[i])\n",
        "\n",
        "          if context_words:\n",
        "              all_out.append(to_categorical(word, V))\n",
        "              all_in.append(context_words)\n",
        "\n",
        "  all_in = sequence.pad_sequences(all_in, maxlen=maxlen, padding='post', value=0)\n",
        "\n",
        "  return (np.array(all_in), np.array(all_out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zzkDpOZCau5d"
      },
      "outputs": [],
      "source": [
        "X_cbow, y_cbow = generate_data_cbow(corpus, window_size, V)\n",
        "X_cbow.shape, y_cbow.shape\n",
        "\n",
        "cbow = Sequential()\n",
        "dim =32\n",
        "cbow.add(Embedding(input_dim=V,\n",
        "                       output_dim=dim,\n",
        "                       input_length=window_size * 2,\n",
        "                       embeddings_initializer='glorot_uniform'))\n",
        "\n",
        "cbow.add(Lambda(lambda x: tf.reduce_sum(x, axis=1), output_shape=(dim,)))\n",
        "\n",
        "cbow.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kiSgGzI4avS0"
      },
      "outputs": [],
      "source": [
        "cbow.compile(optimizer=keras.optimizers.Adam(),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4PFSiQfbUaw",
        "outputId": "e448167c-5aaf-4dce-fb3b-4554fdde2fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0493 - loss: 7.1170\n",
            "Epoch 2/4\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0626 - loss: 5.9788\n",
            "Epoch 3/4\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.0700 - loss: 5.7778\n",
            "Epoch 4/4\n",
            "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0911 - loss: 5.5621\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cbow.fit(X_cbow, y_cbow, batch_size=64, epochs=4, verbose=1)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2GT11LvdAA36"
      },
      "outputs": [],
      "source": [
        "weights = cbow.get_weights()\n",
        "embedding = weights[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "TqSlanNdbZks"
      },
      "outputs": [],
      "source": [
        "f = open(f\"vectors_cbow_{len(embedding[0])}.txt\", \"w\")\n",
        "columns = [\"word\"] + [f\"value_{i+1}\" for i in range(embedding.shape[1])]\n",
        "f.write(\" \".join(columns))\n",
        "f.write(\"\\n\")\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  f.write(word)\n",
        "  f.write(\" \")\n",
        "  f.write(\" \".join(map(str, list(embedding[i,:]))))\n",
        "  f.write(\"\\n\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YCP0Lm3jVsP"
      },
      "source": [
        "**Analogy Computation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FE8rEtQBjO6P"
      },
      "outputs": [],
      "source": [
        "def embed(word, embedding, vocab_size=V, tokenizer=tokenizer):\n",
        "    \"\"\" Embed a word by getting the one hot encoding and taking the dot product of this vector with the\n",
        "        embedding matrix 'word' = string type\n",
        "    \"\"\"\n",
        "    # get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n",
        "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
        "    # get the one-hot encoding of the word\n",
        "    bin_word = to_categorical(int_word, V)\n",
        "    return np.dot(bin_word, embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-8zmSx-hjlRt"
      },
      "outputs": [],
      "source": [
        "def compute_distance(word_a, word_b, word_c, word_d):\n",
        "    \"\"\" Returns the cosine distance between the predicted and the true word (word_d)\n",
        "\n",
        "    Our analogy function is: 'word_a is to word_b as word_c is to ?'\n",
        "    Here, ? is predicted based on the embeddings. Then, we compare ? to word_d, which is the true word.\n",
        "    \"\"\"\n",
        "    models = [skipgram,cbow]\n",
        "    embeddings = [model.get_weights()[0] for model in models]\n",
        "    for embedding in embeddings:\n",
        "        predicted_embedding = embed(word_b, embedding) - embed(word_a, embedding) + embed(word_c, embedding)\n",
        "        dist_exp_true = cosine_distances(predicted_embedding, embed(word_d, embedding))\n",
        "        print(dist_exp_true[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyd5ehUVjwRO",
        "outputId": "1bf0b2a8-7385-4c74-eee1-590832cb2f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.14948418400995245\n",
            "0.5965064309009027\n"
          ]
        }
      ],
      "source": [
        "compute_distance('king', 'queen', 'woman', 'man')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WNDuDNS-nB2f"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine, cdist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "inoLC7AAj199"
      },
      "outputs": [],
      "source": [
        "def get_nearest_words(model_name, embed_word, used_words, nr=10):\n",
        "    \"\"\"Returns the `nr` nearest words to the `embed_word` for a certain `model_name`\n",
        "    \"\"\"\n",
        "    # Load the model embedding matrix and create a list of all the words\n",
        "    df = pd.read_csv(f\"vectors_{model_name}.txt\", sep=\" \")\n",
        "\n",
        "    # Filter out words that are in the analogy\n",
        "    df = df[~(df[\"word\"].isin(used_words))]\n",
        "\n",
        "    # Store the embedded representation of the words\n",
        "    embedded_words = df.iloc[:, 1:].values\n",
        "    embedded_word = embed_word.reshape(1, -1)\n",
        "\n",
        "    # Get the distances between the input embedding and the embedded words such that we can look for the smallest one\n",
        "    # cdist makes it easy for us to compute the cosine distance between each pair of the two collections of inputs\n",
        "    distances = cdist(embedded_word, embedded_words, \"cosine\").reshape(-1)\n",
        "\n",
        "    # Sort distances and store the indices of the `nr` lowest distances\n",
        "    top_sorted_indices = distances.argsort()[:nr]\n",
        "\n",
        "    # Convert the indices to actual words\n",
        "    top_words = [list(df[\"word\"])[i] for i in top_sorted_indices]\n",
        "\n",
        "    # Keep the rounded values of those indices\n",
        "    values = [round(distances[i], 4) for i in top_sorted_indices]\n",
        "    # Concatenate the top words together with their values and return it as a list\n",
        "    return list(zip(top_words, values))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Sn_FuLL4mCxF"
      },
      "outputs": [],
      "source": [
        "def print_analogy(analogy, embeddings, models, model_names, nr=10):\n",
        "    # Retrieve the words from the analogy we need to compute\n",
        "    word_a, word_b, word_c, word_true = analogy\n",
        "\n",
        "    # Formulate the analogy task\n",
        "    analogy_task = f\"{word_a} is to {word_b} as {word_c} is to ?\"\n",
        "\n",
        "    print(f\"Analogy Task: {analogy_task}\")\n",
        "    print(\"---------------------------------------------------\")\n",
        "\n",
        "    # Iterate over all models available\n",
        "    for model_name, embedding in zip(model_names, embeddings):\n",
        "        # Obtain embeddings for all the words\n",
        "        embed_true = embed(word_true, embedding).flatten() # Flatten the embedding\n",
        "        embed_a, embed_b, embed_c = embed(word_a, embedding).flatten(), embed(word_b, embedding).flatten(), embed(word_c, embedding).flatten() # Flatten the embeddings\n",
        "\n",
        "        # Obtain the predicted embedding based on the analogy function\n",
        "        embed_prediction = embed_b - embed_a + embed_c\n",
        "\n",
        "        # The true word with distance similarity value between predicted embedding and true word embedding,\n",
        "        # also denoted `sim1` in the text above\n",
        "        sim1 = round(cosine(embed_true, embed_prediction), 4)\n",
        "\n",
        "        # The predicted word with distance similarity value between predicted embedding and the embedding of the word\n",
        "        # in the vocabulary that is closest to this predicted embedding\n",
        "        word_prediction, sim2 = get_nearest_words(model_name, embed_prediction, [word_a, word_b, word_c], 1)[0]\n",
        "\n",
        "        # Get the top `nr` nearest words\n",
        "        nearest_words = get_nearest_words(model_name, embed_prediction, [word_a, word_b, word_c], nr)\n",
        "\n",
        "        # Print whether or not the true word was in the top nr\n",
        "        partially_correct = word_true in [word[0] for word in nearest_words]\n",
        "\n",
        "\n",
        "        print(f\"Embedding: {model_name}\")\n",
        "        # Print all top nr words with their distance\n",
        "        for word in nearest_words:\n",
        "            print(f\"{word[0]} => {round(word[1], 4)}\")\n",
        "        print(f\"Predicted: {word_prediction} ({round(sim2, 4)}) - True: {word_true} ({sim1})\")\n",
        "        print(f\"Correct? {word_prediction == word_true} - In the top {nr}? {partially_correct}\")\n",
        "        print(\"----------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c85b9814",
        "outputId": "d4c5ac13-a396-4d90-ad63-e71b45ce930d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analogy Task: king is to queen as woman is to ?\n",
            "---------------------------------------------------\n",
            "Embedding: skipgram_32\n",
            "writing => 0.0648\n",
            "till => 0.0677\n",
            "high => 0.0701\n",
            "waited => 0.0767\n",
            "promising => 0.0804\n",
            "wide => 0.0807\n",
            "tucked => 0.081\n",
            "smile => 0.081\n",
            "bottle => 0.0825\n",
            "left => 0.083\n",
            "Predicted: writing (0.0648) - True: man (0.1495)\n",
            "Correct? False - In the top 10? False\n",
            "----------\n",
            "Embedding: cbow_32\n",
            "for => 0.2721\n",
            "knowing => 0.2907\n",
            "explanations => 0.2986\n",
            "kept => 0.3038\n",
            "tureen => 0.3106\n",
            "cool => 0.3107\n",
            "jaws => 0.3115\n",
            "accounts => 0.3129\n",
            "sugar => 0.3162\n",
            "instance => 0.3185\n",
            "Predicted: for (0.2721) - True: man (0.5965)\n",
            "Correct? False - In the top 10? False\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "analogy = ('king', 'queen', 'woman', 'man')\n",
        "\n",
        "models = [skipgram, cbow]\n",
        "embeddings = [model.get_weights()[0] for model in models]\n",
        "model_names = ['skipgram_32', 'cbow_32']\n",
        "\n",
        "print_analogy(analogy, embeddings, models, model_names, nr=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOu7rNjEmZw4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
