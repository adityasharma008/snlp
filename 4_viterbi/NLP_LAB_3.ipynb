{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Aditya Sharma                 \n",
        "\n",
        "Roll Number: 2022BCD0035"
      ],
      "metadata": {
        "id": "5QlzE9If14JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENGLISH CORPUS"
      ],
      "metadata": {
        "id": "ZnhVQzMP4FW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1KruxI00zJ0",
        "outputId": "369027be-8cf9-42f7-cb94-dc549a101672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3914\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "nltk.download(\"treebank\")\n",
        "nltk.download(\"universal_tagset\")\n",
        "\n",
        "tagged_sentences = treebank.tagged_sents(tagset=\"universal\")\n",
        "\n",
        "print(len(tagged_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.7 * len(tagged_sentences))\n",
        "train_data = tagged_sentences[:split_idx]\n",
        "test_data = tagged_sentences[split_idx:]"
      ],
      "metadata": {
        "id": "KSbjeTOV1zbK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7eJOhRxR1zHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_counts = Counter()\n",
        "transition_counts = defaultdict(Counter)\n",
        "emission_counts = defaultdict(Counter)\n",
        "tag_counts = Counter()\n",
        "\n",
        "for sent in train_data:\n",
        "    if sent:\n",
        "        initial_counts[sent[0][1]] += 1\n",
        "    for i in range(len(sent)):\n",
        "        word, tag = sent[i]\n",
        "        tag_counts[tag] += 1\n",
        "        emission_counts[tag][word.lower()] += 1\n",
        "        if i < len(sent) - 1:\n",
        "            next_tag = sent[i + 1][1]\n",
        "            transition_counts[tag][next_tag] += 1\n",
        "\n",
        "all_tags = list(tag_counts.keys())\n",
        "vocab = set(word.lower() for sent in train_data for word, _ in sent)\n",
        "\n",
        "initial_probs = {tag: (initial_counts[tag] + 1) / (len(train_data) + len(all_tags))\n",
        "                 for tag in all_tags}\n",
        "\n",
        "transition_probs = {}\n",
        "for tag in all_tags:\n",
        "    total = sum(transition_counts[tag].values()) + len(all_tags)\n",
        "    transition_probs[tag] = {t: (transition_counts[tag][t] + 1) / total for t in all_tags}\n",
        "\n",
        "emission_probs = {}\n",
        "for tag in all_tags:\n",
        "    total = sum(emission_counts[tag].values()) + len(vocab)\n",
        "    emission_probs[tag] = {w: (emission_counts[tag][w] + 1) / total for w in vocab}"
      ],
      "metadata": {
        "id": "F4fed1JV0_KA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence, initial_probs, transition_probs, emission_probs, all_tags, vocab):\n",
        "    V = [{}]\n",
        "    path = {}\n",
        "\n",
        "    for tag in all_tags:\n",
        "        word = sentence[0].lower()\n",
        "        emission = emission_probs[tag].get(word, 1 / (sum(emission_counts[tag].values()) + len(vocab)))\n",
        "        V[0][tag] = math.log(initial_probs[tag]) + math.log(emission)\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    for t in range(1, len(sentence)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "        word = sentence[t].lower()\n",
        "\n",
        "        for curr_tag in all_tags:\n",
        "            emission = emission_probs[curr_tag].get(word, 1 / (sum(emission_counts[curr_tag].values()) + len(vocab)))\n",
        "            (prob, prev_tag) = max(\n",
        "                (V[t-1][pt] + math.log(transition_probs[pt][curr_tag]) + math.log(emission), pt)\n",
        "                for pt in all_tags\n",
        "            )\n",
        "            V[t][curr_tag] = prob\n",
        "            new_path[curr_tag] = path[prev_tag] + [curr_tag]\n",
        "\n",
        "        path = new_path\n",
        "\n",
        "    (prob, last_tag) = max((V[len(sentence) - 1][tag], tag) for tag in all_tags)\n",
        "    return path[last_tag]"
      ],
      "metadata": {
        "id": "gvsY_iI01Dzp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_data, tagger_func):\n",
        "    total, correct = 0, 0\n",
        "    for sent in test_data:\n",
        "        words = [w for w, _ in sent]\n",
        "        gold_tags = [t for _, t in sent]\n",
        "        pred_tags = tagger_func(words)\n",
        "        total += len(sent)\n",
        "        correct += sum(g == p for g, p in zip(gold_tags, pred_tags))\n",
        "    return correct / total\n",
        "\n",
        "hmm_accuracy = evaluate(test_data, lambda s: viterbi(s, initial_probs, transition_probs, emission_probs, all_tags, vocab))\n",
        "print(f\"HMM Tagger Accuracy: {hmm_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwcjqkA51GRw",
        "outputId": "b04248d7-4279-4fa6-a383-9d98130e5100"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM Tagger Accuracy: 0.8751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm, pos_tag\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "nltk_hmm = trainer.train_supervised(train_data)\n",
        "nltk_hmm_acc = nltk_hmm.evaluate(test_data)\n",
        "print(f\"NLTK HMM Tagger Accuracy: {nltk_hmm_acc:.4f}\")\n",
        "\n",
        "default_tagger = nltk.DefaultTagger(\"NOUN\")\n",
        "default_acc = default_tagger.evaluate(test_data)\n",
        "print(f\"NLTK Default Tagger Accuracy: {default_acc:.4f}\")\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data)\n",
        "unigram_acc = unigram_tagger.evaluate(test_data)\n",
        "print(f\"NLTK Unigram Tagger Accuracy: {unigram_acc:.4f}\")\n",
        "\n",
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "bigram_acc = bigram_tagger.evaluate(test_data)\n",
        "print(f\"NLTK Bigram Tagger Accuracy: {bigram_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4GmuoZC1IP4",
        "outputId": "3fba8cf0-2732-4253-d7ed-d57e33b99233"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3821437922.py:5: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  nltk_hmm_acc = nltk_hmm.evaluate(test_data)\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK HMM Tagger Accuracy: 0.5002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3821437922.py:9: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  default_acc = default_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Default Tagger Accuracy: 0.2915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3821437922.py:13: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  unigram_acc = unigram_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Unigram Tagger Accuracy: 0.8680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3821437922.py:17: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  bigram_acc = bigram_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Bigram Tagger Accuracy: 0.8707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = [\"the\", \"dog\", \"barks\", \"at\", \"the\", \"cat\"]\n",
        "predicted_tags = viterbi(sample_sentence, initial_probs, transition_probs, emission_probs, all_tags, vocab)\n",
        "\n",
        "print(\"\\nSample Sentence Test:\")\n",
        "print(list(zip(sample_sentence, predicted_tags)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSu-G1HT1KBp",
        "outputId": "498c7692-c582-4d17-f11b-38a3f857cf7b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Sentence Test:\n",
            "[('the', 'DET'), ('dog', 'ADJ'), ('barks', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('cat', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HINDI CORPUS"
      ],
      "metadata": {
        "id": "ddg-O3ST4MAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "\n",
        "from conllu import parse_incr\n",
        "\n",
        "hindi_sents = []\n",
        "# https://github.com/UniversalDependencies/UD_Hindi-HDTB/blob/master/hi_hdtb-ud-test.conllu\n",
        "with open(\"/content/hi_hdtb-ud-test.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for tokenlist in parse_incr(f):\n",
        "        sent = []\n",
        "        for token in tokenlist:\n",
        "            if isinstance(token[\"id\"], int):\n",
        "                sent.append((token[\"form\"], token[\"upos\"]))\n",
        "        hindi_sents.append(sent)\n",
        "\n",
        "print(hindi_sents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I0Gcgrw1KgE",
        "outputId": "f1f3f12e-e92b-480c-b38f-312df3d143f3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.12/dist-packages (6.0.0)\n",
            "[('इसके', 'PRON'), ('अतिरिक्त', 'ADP'), ('गुग्गुल', 'PROPN'), ('कुंड', 'PROPN'), (',', 'PUNCT'), ('भीम', 'PROPN'), ('गुफा', 'PROPN'), ('तथा', 'CCONJ'), ('भीमशिला', 'PROPN'), ('भी', 'PART'), ('दर्शनीय', 'ADJ'), ('स्थल', 'NOUN'), ('हैं', 'AUX'), ('।', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(hindi_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3kL-SUr3VKl",
        "outputId": "41dfe501-cb26-45e0-bb6f-23f9047f99f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.7 * len(hindi_sents))\n",
        "\n",
        "train_data = hindi_sents[:split_idx]\n",
        "test_data = hindi_sents[split_idx:]"
      ],
      "metadata": {
        "id": "SNFxQxkt3MSZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def estimate_hmm_params(train_data):\n",
        "    initial_counts = Counter()\n",
        "    transition_counts = defaultdict(Counter)\n",
        "    emission_counts = defaultdict(Counter)\n",
        "    tag_counts = Counter()\n",
        "\n",
        "    for sent in train_data:\n",
        "        if sent:\n",
        "            initial_counts[sent[0][1]] += 1\n",
        "        for i in range(len(sent)):\n",
        "            word, tag = sent[i]\n",
        "            tag_counts[tag] += 1\n",
        "            emission_counts[tag][word] += 1\n",
        "            if i < len(sent) - 1:\n",
        "                next_tag = sent[i + 1][1]\n",
        "                transition_counts[tag][next_tag] += 1\n",
        "\n",
        "    all_tags = list(tag_counts.keys())\n",
        "    vocab = set(word for sent in train_data for word, _ in sent)\n",
        "\n",
        "    initial_probs = {tag: (initial_counts[tag] + 1) / (len(train_data) + len(all_tags))\n",
        "                     for tag in all_tags}\n",
        "\n",
        "    transition_probs = {}\n",
        "    for tag in all_tags:\n",
        "        total = sum(transition_counts[tag].values()) + len(all_tags)\n",
        "        transition_probs[tag] = {t: (transition_counts[tag][t] + 1) / total for t in all_tags}\n",
        "\n",
        "    emission_probs = {}\n",
        "    for tag in all_tags:\n",
        "        total = sum(emission_counts[tag].values()) + len(vocab)\n",
        "        emission_probs[tag] = {w: (emission_counts[tag][w] + 1) / total for w in vocab}\n",
        "\n",
        "    return initial_probs, transition_probs, emission_probs, all_tags, vocab\n"
      ],
      "metadata": {
        "id": "1OMe0b-w3oGR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_probs, transition_probs, emission_probs, all_tags, vocab = estimate_hmm_params(train_data)\n",
        "\n",
        "hmm_accuracy = evaluate(\n",
        "    test_data,\n",
        "    lambda s: viterbi(s, initial_probs, transition_probs, emission_probs, all_tags, vocab)\n",
        ")\n",
        "print(f\"HMM Hindi Tagger Accuracy: {hmm_accuracy:.4f}\")\n",
        "\n",
        "sample_sentence = [\"राम\", \"स्कूल\", \"जाता\", \"है\"]\n",
        "predicted_tags = viterbi(sample_sentence, initial_probs, transition_probs, emission_probs, all_tags, vocab)\n",
        "\n",
        "print(\"\\nSample Sentence Prediction:\")\n",
        "print(list(zip(sample_sentence, predicted_tags)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr-sQrwl2VyM",
        "outputId": "19dea5dc-aa3f-40e1-cc14-81e34669b67d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM Hindi Tagger Accuracy: 0.7866\n",
            "\n",
            "Sample Sentence Prediction:\n",
            "[('राम', 'PROPN'), ('स्कूल', 'VERB'), ('जाता', 'AUX'), ('है', 'AUX')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNDkIoT_3doL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}