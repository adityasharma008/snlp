{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQyQjeA4RXzA",
        "outputId": "151172b6-081d-4461-8bfe-ac6082a84da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import math\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = gutenberg.raw('austen-emma.txt')\n",
        "# raw_text = gutenberg.raw('carroll-alice.txt')\n",
        "tokens = nltk.word_tokenize(raw_text)\n",
        "tokens = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "train_size = int(len(tokens) * 0.8)\n",
        "train_tokens = tokens[:train_size]\n",
        "test_tokens = tokens[train_size:]\n",
        "\n",
        "vocab = set(train_tokens)\n",
        "V = len(vocab)"
      ],
      "metadata": {
        "id": "IjsDfMPMResO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(tokens, n):\n",
        "    model = defaultdict(Counter)\n",
        "    if n == 1:\n",
        "        model['unigram'] = Counter(tokens)\n",
        "        total_count = float(sum(model['unigram'].values()))\n",
        "        for word in model['unigram']:\n",
        "            model['unigram'][word] /= total_count\n",
        "        return model\n",
        "\n",
        "    ngrams_list = list(ngrams(tokens, n))\n",
        "    context_list = list(ngrams(tokens, n-1))\n",
        "\n",
        "    ngram_counts = Counter(ngrams_list)\n",
        "    context_counts = Counter(context_list)\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        context = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        model[context][word] = (count + 1) / (context_counts[context] + V)\n",
        "\n",
        "    for context in context_counts:\n",
        "        unseen_words = V - len(model[context])\n",
        "        default_prob = 1 / (context_counts[context] + V)\n",
        "        model[context]['<UNK>'] = default_prob * unseen_words\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(model, n, start_word, num_words=10):\n",
        "    if n == 1:\n",
        "        words = list(model['unigram'].keys())\n",
        "        probs = list(model['unigram'].values())\n",
        "        return ' '.join(random.choices(words, weights=probs, k=num_words))\n",
        "\n",
        "    text = [start_word]\n",
        "    current_context = [start_word]\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        context_tuple = tuple(current_context[-(n-1):])\n",
        "\n",
        "        if context_tuple in model:\n",
        "            next_word_probs = model[context_tuple]\n",
        "            words = list(next_word_probs.keys())\n",
        "            probs = list(next_word_probs.values())\n",
        "\n",
        "            total_prob = sum(probs)\n",
        "            if total_prob > 0:\n",
        "                probs = [p / total_prob for p in probs]\n",
        "            else:\n",
        "                next_word = random.choice(list(vocab))\n",
        "                text.append(next_word)\n",
        "                current_context.append(next_word)\n",
        "                continue\n",
        "\n",
        "            next_word = random.choices(words, weights=probs, k=1)[0]\n",
        "            if next_word == '<UNK>':\n",
        "                next_word = random.choice(list(vocab))\n",
        "\n",
        "        else:\n",
        "            next_word = random.choice(list(vocab))\n",
        "\n",
        "        text.append(next_word)\n",
        "        current_context.append(next_word)\n",
        "\n",
        "    return ' '.join(text)\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, n, test_tokens):\n",
        "    log_prob_sum = 0\n",
        "    test_ngram_count = 0\n",
        "\n",
        "    if n == 1:\n",
        "        test_ngram_list = test_tokens\n",
        "        unigram_probs = model['unigram']\n",
        "        for token in test_ngram_list:\n",
        "            prob = unigram_probs.get(token, 1/V)\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log(prob)\n",
        "        test_ngram_count = len(test_ngram_list)\n",
        "    else:\n",
        "        test_ngram_list = list(ngrams(test_tokens, n))\n",
        "        for ngram in test_ngram_list:\n",
        "            context = ngram[:-1]\n",
        "            word = ngram[-1]\n",
        "\n",
        "            if context in model and word in model[context]:\n",
        "                prob = model[context][word]\n",
        "            elif context in model and '<UNK>' in model[context]:\n",
        "                 prob = model[context]['<UNK>']\n",
        "            else:\n",
        "                prob = 1 / V\n",
        "\n",
        "            if prob > 0:\n",
        "                log_prob_sum += math.log(prob)\n",
        "\n",
        "        test_ngram_count = len(test_ngram_list)\n",
        "\n",
        "    if test_ngram_count == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / test_ngram_count)\n",
        "    return perplexity\n"
      ],
      "metadata": {
        "id": "SUCIYKu3Rlen"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_val in range(1, 5):\n",
        "    print(f\"N-Gram Model: n={n_val}\")\n",
        "\n",
        "    ngram_model = train_model(train_tokens, n_val)\n",
        "\n",
        "    start_word = random.choice(train_tokens)\n",
        "    generated_sequence = generate_text(ngram_model, n_val, start_word, num_words=10)\n",
        "    print(f\"Generated Text: {generated_sequence}\")\n",
        "\n",
        "    perplexity = calculate_perplexity(ngram_model, n_val, test_tokens)\n",
        "    print(f\"Perplexity: {perplexity:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_MKGSYGRuY9",
        "outputId": "94d9b24a-14d1-4d05-eda3-12ae6a53ce0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-Gram Model: n=1\n",
            "Generated Text: he of to equal one or and did something passing\n",
            "Perplexity: 478.0174\n",
            "\n",
            "N-Gram Model: n=2\n",
            "Generated Text: difference bearing many establish touch fixing informs doing channel simple knowing\n",
            "Perplexity: 84.4412\n",
            "\n",
            "N-Gram Model: n=3\n",
            "Generated Text: of lot cultivation giving doubtfully want satisfied allay spinet stands commandingly\n",
            "Perplexity: 98.9957\n",
            "\n",
            "N-Gram Model: n=4\n",
            "Generated Text: wished seriously perry cousin william objecting naming wrong equal specimen severity\n",
            "Perplexity: 1245.2867\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yVmcRcndRu9_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OW0fiqEb06R"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}